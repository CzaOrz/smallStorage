run_download函数，实际上还是执行的scrpy框架，公司就是在pipeline管道上做了处理。自定义了item，来与值进行过配合
自己写的部分，有start_requets，start_urls，item的一些基本定义，然后就是对获取到源码的返回做了一个标准化的输出
也就是那个urk_yield，这个很重要，对各种下载后的细节进行初步的分类，然后在yield到pipeline中进行存储，这一步没有什么难度，基本也都是scrapy自带的东西

从执行命令开始，运行execute函数，转到cmdline文件中，首先会收集scrapy的settings设置，然后通过sys获取输入的信息
找到对应的命令，加载对应的命令函数，如scrawl.py函数，然后通过settings实例化一个CrawlerProcess对象
初始化这个对象，并执行队名cmd函数的run方法，运行cmd函数。在实际的cmd函数中，起作用的是两个函数，即self.crawler_process.crawl和self.crawler_process.start方法
后一个方法实际就是调用前一个方法获取到的Deferred对象，这个对象保存在self._active队列中，将其钱全部取出，并添加好对象回调函数，然后执行reactor.run()方法即可以区启动整个函数了
现在说一下前面的方法，这个crawl方法是在CrawlerRunner中实现的，作用是通过settings和爬虫的名字，实例化一个Crawler对象，这个对象是他们的父类。从名字中大概也可以看出来了
CrawlerProcess作用是管理各个Crawler工作的，CrawlerRunner是各个Crawler运行的一些情况，而Crawler则是对每一个任务的描述，这是一个完整的爬虫管理个体
实例化Crawler对象后执行self._crawl函数，这个函数，获取Crawler.crawl()函数执行后的Deferred对象，然后将d添加带self._active队列中，设置一些回调然后没了
主要看Crawler对象中的crawl函数做了些啥事，这个函数通过settings创建了一个爬虫然后再创建了一个引擎，然后通过爬虫的start_request方法获取到了对应的Request生成器
然后激活引擎，也就是self.engine.open_spider()这里会传入一些参数，有spider和start_request等
我们再来看看引擎中的open_spider函数怎么执行的，这个函数构造了一个nextcall函数，这个函数与self._next_request相关联，一旦我们执行nextcall.schedule()函数，就会立即执行一次self.next_request函数，使用了python的__call__魔术方法，暂时先不管这个函数的具体实现，往后看看这个引擎的激活函数做了写啥
然后实例化一个调度器，再通过scrapy中的spidermw.process_start_reqquest方法对start_request进行初步的处理，这个处理不是中间件的处理，是爬虫请求到引擎哪一步的请求
然后实例化一个Slot函数，这个函数是当前文件的一个管理者，或者是一个生命周期的映射。然后激活调度器，激活scraper，激活状态记录器，
这三个已激活，就通过pypatch进行消息传递，广播一个spider_open的消息，所有关注这个消息的函数都会执行一些响应的操作
然后就是两个比较迷的操作了，直接就执行了一个nextcall，schedule()函数，这是想干啥，主动触发一次吗=0=然后就设置了一个5秒的心跳函数，我觉得这个主动触发应该是当所有的reactor.run开始执行后，跑到这就会开始执行，这个执行是一个恶性的无线循环，估计会把队列里面的所有request请去请求完才会做别的处理把
是不是他完整的流程都体现在这个self.next_request函数力啊，这个函数，会不停的对一些初始消息做遍历与判断，一但达到某些条件我们就会往后走，这里就是，比如这个函数，执行函数self.get_next_reqquest_from_schedule，先从调度器中国清去除一个reqquest，
然后将这个歌request传入self._download进行下载，其实也不是下载，他首先还是会传到中间件进行处理，这里就是使用了fetch，这个fetch来自self.download.fetch()函数，这里就是走到downloader文件中，执行fetch函数，这儿的download对象中，也知识做了一个中转，也就是说具体的实现者还是self.middleware.download函数，
虽然说具体的下载逻辑还是在handle中，但在传递到中间件的，还有一个回调函数，这个函数不得掉，就是调用具体下载函数的，这个函数enqqueue_reuqest，这个函数就和他名字一个，就是将数据推到queue队列中没然后执行self._process-request函数，然后不用说，这个函数第一步就是从queue中却出队列
这样获取也是有原因的，可以在上一个函数中获取一个个Deferred对象，然后传到这边来，这边直接chainDeferred将其连接在后面，那样就好比所有的逻辑都是承接在我现在的代码之后，这样阅读起来就更改时方便？反正我是不觉得。早self._process_request函数中，执行官self._download函数，这个函数，是这个类中的主逻辑函数他执行了self.handler.download——request函数，这是就会跳转到handle类中，
这个dowmload_reqyest函数就是先根据你的类别，来加载最适合的下载器，一般我们还是选择TextResponse较多一点，其他的我也不好说，在这里我们可以看到，似乎每一个类中都是self._download函数比较中啊哟，而且一般都没有做事情请，而是在这一步进行了一些关键性额跳转逻辑
在真正的hadnler指定的下载器中在此执行download_request函数，这个函数使用的下载是Agent.request下载，这是一个Twisted的下在方式，我也看不出这有啥好坏
这个函数，Agent.request(method,url,header,body)，主要还是这四类，然后下载好之后，会执行三个回调函数，第一个不重要，就是几率一下下载的时间把，最后一个硬广告是最重要的，他讲下载后的结果撞到了TextRespobse类中，然后我们可以使用replace，xpath，css，url，ecoding等很多骚方法，这些都是scrapy带给我们的方法与优势
只玩这些后买几本我们就可以得到一个response结果了，但是我们还需要走最后异步，也就是scraper，进入之前我们设定的会带哦函数，enqueue_scraper，这个函数就直接跑到了scrape中，在这个函数中没回设定一些会带哦函数，都是老套路，然后执行self._scrape_next函数，当然，肯定是已经入队了，然后我们在scrape-next中可以执行接下来的函数
遍历整个队列，获取之后，将其添加到一个个List中，然后一起执行，执行self._scrape函数，这个函数貌似有两部，一个self._scrape和一个self._scrape2，好吧，以啥也没错，就是添加了一些回调函数而已，然后我们坎儿做了啥事，2调用了spidermw中间件进行处理，这个不是pipeline的处理把，这个是response从引擎重新回到spider中是需要经过的一个不走，虽然一般没怎么关心
